# Advanced CNN Architecture 

Convulational Neural Networks (CNNs) have significantly advanced the field of computer vision. Understanding these architecture is crucial for tasks such as image classification, object detection, and semantic segmentation. 

## Key CNN Architecture 
1. **LeNet-5**
LeNet is the first CNN architecture and an example of a gradient-based learning.

LeNet was trained on the Modified NIST or MNIST data set and designed to identify handwritten numbers on checks. The weights and hyperparameters are varied so that the gradient divergence on the loss function reaches a minimum. But as used in computer vision, the weights and hyperparameters in LeNet were engineered manually. The input for LeNet is 32Ã—32, which is visible without magnification and far larger than the size of the characters as originally written. The large input allows for minute features from the image to be captured.

LeNet consists of convolution layers, subsampling layers and fully connected layers.
- The convolution layers extract the features from the images.
- The subsampling layer consists of applying the activation function to the input from convolution layers and performing the pooling process on the output obtained from applying activation function.
- A fully connected layer connects each neuron of one layer to every neuron of another layer. The last layer of the fully connected layer is reserved for an activation function to assist in classification.

2. **AlexNet**
Introduced deeper networks with ReLU activation and dropout, achieving significant improvements in image classification tasks.

3. **VGGNET**
Known for its simplicity and depth, using small convulational filter (3x3) and increasing depth to improve feature extraction.

4. **Inception (GoogLeNet)**
Feature inception modules that apply multiple convolutional filters of different sizes in parallel, capturing various features at different scales. 

5. **ResNet (Residual Network)**
Introduces skip connections to allow gradient to flow through deeper networks, mitigating the vanishing gradient problem and enabling the training of very deep networks. 

6. **Xception**
Combines depthwise separable convolutions with inception modules, improving efficieny and performance. 

7. **MobileNet**
Designed for mobile and embedded vision application, focusing on lightweight models with reduced computational requirements.

## Advanced Training Techniques
To further improve the performance and generalization of CNNs, advanced training techniques are employed:

- Transfer Learning: Learn how to leverage pre-trained CNN models, fine-tuning them for specific tasks, saving training time and resources.
- Data Augmentation: Discover how data augmentation can increase the diversity of the training data, reducing overfitting and improving the model's robustness.

## Transfer Learning 
Transfer learning is a powerful technique that allows practitioners to leverage pre-trained models on large datasets for specific tasks. Transfer learning is a machine learning technique that involves reusing a pre-trained model developed for one task to improve performance on a different but related task. This approach saves time and resources, as it requires less data and training time. 

Key steps include:
1. **Select a Pre-trained Model**: Choose a pre-trained CNN model that suits the task and dataset. Popular choices include VGG, ResNet, Inception, or MobileNet. These models are typically available in deep learning libraries like TensorFlow or PyTorch.
2. **Load Pre-trained Model**: Load the pre-trained CNN model without the top (fully-connected) layers. This allows us to leverage the pre-trained model's learned features.
3. **Customize the Model**: Add new layers on top of the pre-trained model to adapt it to your specific task. These layers should include a suitable architecture for your task, such as fully-connected layers, dropout layers, or convolutional layers. Adjust the number of neurons or classes based on your specific requirements.
4. **Freeze Pre-trained Layers**: Freeze the weights of the pre-trained layers to prevent them from being updated during training. This ensures that the pre-trained features are retained and not modified.
5. **Prepare Data**: Preprocess your dataset according to the input requirements of the pre-trained model. This may involve resizing, normalizing, or augmenting the images.
6. **Train the Model**: Train the model using your dataset. Only the newly added layers on top of the pre-trained model will be trained while the pre-trained layers remain frozen.
7. **Fine-tuning (Optional)**: If you have sufficient data and want to further improve performance, you can unfreeze some of the pre-trained layers and fine-tune them along with the new layers. This allows the model to adapt to the specific features of your dataset.
8. **Evaluate and Test**: Evaluate the trained model using validation data or cross-validation techniques. Measure metrics such as accuracy, loss, precision, or recall to assess performance. Finally, test the model on unseen data to get an estimate of its real-world performance.

## Benefits of Transfer Learning with CNNs 
1. **Reduced Training Time and Computational Resources**: Training a CNN from scratch on large datasets can be computataionally expensive. Transfer learning enables us to leverage pre-trained models, saving significant time and resources. 
2. **Improved Performance with Limited Data**: By utilizing pre-trained models, we can overcome the data scarcity problem. The pre-trained models have learned generic features from large-scale datasets, which can be beneficial when working with limited labeled data. 
3. **Generalization to New Tasks**: Transfer learning allows us to transfer knowledge learned from one task to another. The feature learned from a large dataset are often general enough to be applicable to various related tasks. 
4. **Faster Convergence**: Transfer learning can lead to faster convergence during training, as the pre-trained model has already learned useful features, reducing the number of iterations required to reach optimal performance. 

## Resources 
- [Computer Vision CNN Architectures](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-4/computer-vision-cnn-architectures.html?product=1601111740010412)
- [Transfer Learning Alexnet](https://github.com/krishnaik06/Advanced-CNN-Architectures/blob/master/Transfer%20Learning%20Alexnet.ipynb)
